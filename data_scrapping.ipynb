{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdeab664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error for URL_ID blackassign0036: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Error for URL_ID blackassign0049: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd \n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "\n",
    "data_frame = pd.read_excel(\"input.xlsx\")\n",
    "\n",
    "for index, row in data_frame.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        html_content = response.content\n",
    "        corpus=[]\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        title = soup.find('h1').text.strip()\n",
    "        \n",
    "        article_content_div = soup.find('div', class_='td-post-content')\n",
    "\n",
    "        article_content = \"\"\n",
    "\n",
    "        if article_content_div:\n",
    "            paragraphs = article_content_div.find_all(['p', 'li', 'ul', 'ol'])\n",
    "            \n",
    "            article_content = '\\n'.join([paragraph.get_text(strip=True) for paragraph in paragraphs if paragraph.get_text(strip=True)])\n",
    "        \n",
    "        os.makedirs('extracted1',exist_ok=True)\n",
    "        with open(f\"extracted\\{url_id}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(title + \"\\n\\n\")\n",
    "            file.write(article_content)\n",
    "            \n",
    "            \n",
    "            \n",
    "    except requests.exceptions.HTTPError as error:\n",
    "        print(f\"Error for URL_ID {url_id}: {error}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "20e56a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text file for URL_ID blackassign0036 not found. Using placeholders...\n",
      "Text file for URL_ID blackassign0049 not found. Using placeholders...\n"
     ]
    }
   ],
   "source": [
    "stopword_dir=\"StopWords\"\n",
    "text_dir='extracted'\n",
    "master_dict='MasterDictionary'\n",
    "\n",
    "def load_stopwords(stopword_dict):\n",
    "    stopwords=set()\n",
    "    for file in os.listdir(stopword_dict):\n",
    "        if file.endswith('.txt'):\n",
    "            with open(os.path.join(stopword_dict,file),'r',encoding='latin1') as f:\n",
    "                stopwords.update(f.read().splitlines())\n",
    "    return stopwords\n",
    "\n",
    "stop_words=load_stopwords(stopword_dir)\n",
    "\n",
    "with open('MasterDictionary/positive-words.txt','r') as file:\n",
    "    positive_list=[line.strip() for line in file.readlines()]\n",
    "with open('MasterDictionary/negative-words.txt','r') as file:\n",
    "    negative_list=[line.strip() for line in file.readlines()]\n",
    "    \n",
    "\n",
    "\n",
    "def count_syllables(word):\n",
    "    vowels = \"aeiouy\"\n",
    "    count = 0\n",
    "    word = word.lower()\n",
    "    if word[0] in vowels:\n",
    "        count += 1\n",
    "    for index in range(1, len(word)):\n",
    "        if word[index] in vowels and word[index - 1] not in vowels:\n",
    "            count += 1\n",
    "    if word.endswith('e'):\n",
    "        count -= 1\n",
    "    if count == 0:\n",
    "        count += 1 \n",
    "    return count\n",
    "\n",
    "with open('extracted/blackassign0001.txt', 'r') as file:\n",
    "    text_data = file.read()\n",
    "\n",
    "def analyze_text(text,stop_words,positive_list,negative_list):\n",
    "    sentences=sent_tokenize(text)\n",
    "    words=nltk.word_tokenize(text)\n",
    "    \n",
    "    word_clean=[word for word in words if word.isalpha() and word.lower() not in stop_words]\n",
    "    \n",
    "    word_count=len(word_clean)\n",
    "    \n",
    "    sentences_count=len(sentences)\n",
    "    \n",
    "    complex_wordcount=sum(1 for word in word_clean if count_syllables(word)>=3)\n",
    "    \n",
    "    syllables=sum(count_syllables(word) for word in word_clean)\n",
    "    \n",
    "    personal_pronouns = len(re.findall(r'\\b(?:I|we|my|ours|us)(?!US\\b)\\b', text, flags=re.IGNORECASE))\n",
    "\n",
    "    \n",
    "    positive_words=len([word for word in word_clean if word.lower() in positive_list])\n",
    "    negative_words=len([word for word in word_clean if word.lower() in negative_list])\n",
    "    \n",
    "    avg_sent_length=word_count/sentences_count if sentences_count else 0\n",
    "    \n",
    "    per_complex_word=(complex_wordcount/word_count*100) if word_count else 0\n",
    "    \n",
    "    fog_index=0.4*(avg_sent_length+per_complex_word)\n",
    "    \n",
    "    \n",
    "    avg_word_length=sum(len(word) for word in word_clean)/word_count if word_count else 0\n",
    "    \n",
    "    avg_words_per_sentence = word_count / sentences_count if sentences_count else 0\n",
    "\n",
    "    polarity_score=(positive_words-negative_words)/((positive_words+negative_words)+0.000001)\n",
    "    \n",
    "    sub_score=(positive_words+ negative_words)/((word_count)+0.000001)\n",
    "   \n",
    "\n",
    "    return {\n",
    "        'POSITIVE SCORE': positive_words,\n",
    "        'NEGATIVE SCORE': negative_words,\n",
    "        'POLARITY SCORE': polarity_score,\n",
    "        'SUBJECTIVITY SCORE': sub_score,\n",
    "        'AVG SENTENCE LENGTH': avg_sent_length,\n",
    "        'PERCENTAGE OF COMPLEX WORDS': per_complex_word,\n",
    "        'FOG INDEX': fog_index,\n",
    "        'AVG NUMBER OF WORDS PER SENTENCE':avg_words_per_sentence,\n",
    "        'COMPLEX WORD COUNT': complex_wordcount,\n",
    "        'WORD COUNT': word_count,\n",
    "        'SYLLABLE PER WORD': syllables / word_count if word_count else 0,\n",
    "        'PERSONAL PRONOUNS': personal_pronouns,\n",
    "        'AVG WORD LENGTH': avg_word_length\n",
    "        \n",
    "\n",
    "    }\n",
    "    \n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "for index, row in data_frame.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    try:\n",
    "        with open(f\"extracted/{url_id}.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "            text = file.read()\n",
    "            analysis_results = analyze_text(text, stop_words, positive_list, negative_list)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Text file for URL_ID {url_id} not found. Using placeholders...\")\n",
    "        # Set placeholder values for analysis results\n",
    "        analysis_results = {\n",
    "            'POSITIVE SCORE': 0,\n",
    "            'NEGATIVE SCORE': 0,\n",
    "            'POLARITY SCORE': 0,\n",
    "            'SUBJECTIVITY SCORE': 0,\n",
    "            'AVG SENTENCE LENGTH': 0,\n",
    "            'PERCENTAGE OF COMPLEX WORDS': 0,\n",
    "            'FOG INDEX': 0,\n",
    "            'AVG NUMBER OF WORDS PER SENTENCE': 0,\n",
    "            'COMPLEX WORD COUNT': 0,\n",
    "            'WORD COUNT': 0,\n",
    "            'SYLLABLE PER WORD': 0,\n",
    "            'PERSONAL PRONOUNS': 0,\n",
    "            'AVG WORD LENGTH': 0\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL_ID {url_id}: {e}\")\n",
    "        # Set placeholder values for analysis results\n",
    "        analysis_results = {\n",
    "            'POSITIVE SCORE': None,\n",
    "            'NEGATIVE SCORE': None,\n",
    "            'POLARITY SCORE': None,\n",
    "            'SUBJECTIVITY SCORE': None,\n",
    "            'AVG SENTENCE LENGTH': None,\n",
    "            'PERCENTAGE OF COMPLEX WORDS': None,\n",
    "            'FOG INDEX': None,\n",
    "            'AVG NUMBER OF WORDS PER SENTENCE': None,\n",
    "            'COMPLEX WORD COUNT': None,\n",
    "            'WORD COUNT': None,\n",
    "            'SYLLABLE PER WORD': None,\n",
    "            'PERSONAL PRONOUNS': None,\n",
    "            'AVG WORD LENGTH': None\n",
    "        }\n",
    "    \n",
    "    # Append analysis results to the results list\n",
    "    results.append({**row.to_dict(), **analysis_results})\n",
    "\n",
    "# Create a DataFrame from the results and write it to an Excel file\n",
    "    results_df = pd.DataFrame(results)\n",
    "    results_df.to_excel(\"Output Data Structure.xlsx\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4158107a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
